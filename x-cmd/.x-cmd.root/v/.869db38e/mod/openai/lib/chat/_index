# this is a moduel to honor x-bash/chat module
# shellcheck shell=dash disable=SC2034
xrc chat
xrc:mod:lib     openai      chat/send chat/exec

___x_cmd_openai_chat(){
    local X_help_cmd='x help -m openai chat'; help:arg:parse
    local op="$1";
    case "$op" in
        request|preparehistory|exec)
            shift; ___x_cmd_openai_chat_"$op" "$@" ;;
        *)  N=openai M="Not support such [subcmd=$op]" log:ret:64
    esac
}

___x_cmd_openai_chat_request(){
    local X_help_cmd='x help -m openai chat request'; help:arg:parse
    ___x_cmd chat --exec --provider openai "$@"
}

___x_cmd_openai_chat_request___launch(){
    local content_dir
    read -r content_dir
    [ -d "$content_dir" ] || return 1

    ___x_cmd_openai_chat_request___trapexit(){
        openai:debug "Remove chat.running file"
        ___x_cmd rmrf "$content_dir/chat.running"
    }

    printf "%s\n" $$ >"$content_dir/chat.running"
    trap '___x_cmd_openai_chat_request___trapexit' EXIT

    {
        local model
        read -r model

        local request_body_file="$content_dir/openai.request.body.yml"
        [ -f "$request_body_file" ] || return

        ___x_cmd retry --max 2 --interval 3 ___x_cmd_openai_chat_request___try
    } || {
        ___x_cmd_openai_chat_request___trapexit
        return 1
    }
}

___x_cmd_openai_chat_request___try(){
    openai:debug "Sending request to openai server"
    [ -z "$confirm_before_send" ] || {
        < "$request_body_file" ___x_cmd j2y | ___x_cmd bat -l yml --wrap never >/dev/tty
        ___x_cmd ui yesno "Do your want to send this message?" || {
            ___X_CMD_RETRY_ABORT=1
            return 1
        }
        confirm_before_send=
    }

    local _md5; _md5="$( < "$request_body_file" ___x_cmd str md5 )"
    {
        request_body_file="$request_body_file" \
        ___x_cmd ccmd "$cache_time" -- \
            ___x_cmd_openai_request_generaxwtecontent "$model" "$_md5"
    } | {
        local interative=
        if ___x_cmd_is_interactive; then interative=1; fi

        local errcode=0
        ___x_cmd cawk -i -E content_dir,interative         \
                -m j/json,j/jiter,j/jcp                                     \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/history.awk"            \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"               \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"             \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"               \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/creq.awk"               \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"           \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai_stream_output_util.awk"    \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_response.awk" || {
            errcode=$?
            ___x_cmd ccmd - ___x_cmd_openai_request_generaxwtecontent "$model" "$_md5"
        }

        case $errcode in
            0)  ;;
            2)  ___X_CMD_RETRY_ABORT=1; errcode=1 ;;
            *)  ___x_cmd rmrf "$content_dir/openai.response.yml" "$content_dir/chat.response.yml" ;;
        esac
        return "$errcode"
    }
}

# ___x_cmd openai chat --model gpt4 -n 3 --file a.md -f b.md -f c.md --prompt english ''

# @gpt3 -n 3 -f a.md -f b.md -p en --
# @gpt3 -n 3 -f a.md -f c.jd -p cn --
# @gpt4

# using control command for this like vscode

# ___x_cmd chat start

# ___x_cmd chat start ==> create a new thread
# ___x_cmd chat set 3 ==> set history to 3
# @gpt3 how to understand

# @ws /start
# @ws /set 3

# ___x_cmd chat history cp ''

# @en
# @cn

