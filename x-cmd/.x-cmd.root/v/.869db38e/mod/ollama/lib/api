


# endpoint
___x_cmd_ollama_api_isavilable(){
    ___x_cmd curl ""
    # ps to acheive a model that can be used ...
}


# ___x_cmd_ollama_api_handle(){
#     :
# }

# using json mode
# using other mode
# using stream mode ...
# ___x_cmd_ollama_api_generate(){
#     :
# }

# using streaming mode ...
# https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion
# ___x_cmd_ollama_api_chat(){
#     :
# }


