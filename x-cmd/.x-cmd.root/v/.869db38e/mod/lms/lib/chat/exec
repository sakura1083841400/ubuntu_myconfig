# shellcheck shell=dash

___x_cmd_lms_chat_exec(){
    local cfg_model=;       local cfg_maxtoken=;    local cfg_seed=;    local cfg_temperature=
    ___x_cmd_lms_cfg --var cfg_model=model cfg_maxtoken=maxtoken cfg_seed=seed cfg_temperature=temperature 2>/dev/null

    lms:debug --cfg_model "$cfg_model" --cfg_maxtoken "$cfg_maxtoken" --cfg_seed "$cfg_seed" --cfg_temperature "$cfg_temperature" "chat request"

    {
        provider_name=lms \
        ___x_cmd cawk  -E ___X_CMD_CHAT_SESSION_DIR,session,history_num,minion,model,chatid,minion_json_cache,system,type,filelist_attach,temperature   \
                -E cfg_history_num,cfg_session,cfg_minion,cfg_model,cfg_maxtoken,cfg_seed,cfg_temperature \
                -m j/json,j/jiter,j/jcp \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/history.awk"        \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"         \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/creq.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"           \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"       \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_request.awk" <<A
${question}
${___X_CMD_UNSEENCHAR_001}${___X_CMD_UNSEENCHAR_002}${___X_CMD_UNSEENCHAR_003}:image
${imagelist}
A
    } | ( ___x_cmd_lms_chat_request___launch )
}

